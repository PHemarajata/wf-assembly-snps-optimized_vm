process.shell = ['/bin/bash']



mail {
    smtp.host = 'smtpgw.cdc.gov'
}

profiles {
    test {
        params.inpath   = "${PWD}/tests/data"
        params.outpath  = "${PWD}/test-outpath"
        params.logpath  = "${PWD}/test-outpath/log"
    }

    sge {
        process {
            executor = 'sge'
            penv     = 'smp'
            queue    = 'all.q'
        }
        executor {
            queueSize    = 100
            pollInterval = '15 sec'
        }
    }

    short {
        process {
            queue = 'short.q'
        }
    }

    gpu {
        process {
            queue = 'gpu.q'
        }
    }

    highmem {
        process {
            queue = 'highmem.q'
        }
    }

    conda {
        docker.enabled         = false
        params.enable_conda    = true
        singularity.enabled    = false
        shifter.enabled        = false
    }

    docker {
        docker.enabled         = true
        docker.userEmulation   = true
        fixOwnership           = true
        runOptions             = "-u \$(id -u):\$(id -g)"
        singularity.enabled    = false
        shifter.enabled        = false
        registry               = "snads"
    }

    singularity {
        singularity.enabled    = true
        singularity.autoMounts = true
        singularity.cacheDir   = "${baseDir}/assets"
        docker.enabled         = false
        shifter.enabled        = false
        conda.enabled          = false
        params.enable_conda    = false
    }

    shifter {
        shifter.enabled        = true
        docker.enabled         = false
        singularity.enabled    = false
    }
}

// Function to ensure that resource requirements don't go beyond a maximum limit
// This code is from: https://github.com/nf-core/rnaseq/blob/3643a94411b65f42bce5357c5015603099556ad9/nextflow.config
def check_max(obj, type) {
  if (type == 'memory') {
    try {
      if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
        return params.max_memory as nextflow.util.MemoryUnit
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
      return obj
    }
  } else if (type == 'time') {
    try {
      if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
        return params.max_time as nextflow.util.Duration
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
      return obj
    }
  } else if (type == 'cpus') {
    try {
      return Math.min( obj, params.max_cpus as int )
    } catch (all) {
      println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
      return obj
    }
  }
}

// This code is from: https://github.com/nf-core/viralrecon/blob/master/conf/base.config
process {

    cpus   = { check_max( 1    * task.attempt, 'cpus'   ) }
    memory = { check_max( 6.GB * task.attempt, 'memory' ) }
    time   = { check_max( 4.h  * task.attempt, 'time'   ) }

    errorStrategy = { task.exitStatus in [143,137,104,134,139] ? 'retry' : 'finish' }
    maxRetries    = 1
    maxErrors     = '-1'

    // Process-specific resource requirements
    withLabel:process_low {
        cpus   = { check_max( 2      * task.attempt, 'cpus'   ) }
        memory = { check_max( 12.GB  * task.attempt, 'memory' ) }
        time   = { check_max( 4.h    * task.attempt, 'time'   ) }
    }
    withLabel:process_medium {
        cpus   = { check_max( 6      * task.attempt, 'cpus'   ) }
        memory = { check_max( 36.GB  * task.attempt, 'memory' ) }
        time   = { check_max( 8.h    * task.attempt, 'time'   ) }
    }
    withLabel:process_high {
        cpus   = { check_max( 12     * task.attempt, 'cpus'   ) }
        memory = { check_max( 72.GB  * task.attempt, 'memory' ) }
        time   = { check_max( 16.h   * task.attempt, 'time'   ) }
    }
    withLabel:process_long {
        time   = { check_max( 20.h   * task.attempt, 'time'   ) }
    }
    withLabel:process_high_memory {
        memory = { check_max( 200.GB * task.attempt, 'memory' ) }
    }
    withLabel:error_ignore {
        errorStrategy = 'ignore'
    }
    withLabel:error_retry {
        errorStrategy = 'retry'
        maxRetries    = 2
    }
}

// def trace_timestamp = new java.util.Date().format('yyyy-mon-dd_dow_HH:mm:ss')
def trace_timestamp = new java.util.Date().format('yyyy-MM-dd HH:mm:ss')

// Default parameters (can be overwritten by command line inputs)
params {
    inpath = new File("${launchDir}").getCanonicalPath()
    outpath = new File("${launchDir}").getCanonicalPath()
    logpath = new File("${params.outpath}/.log").getCanonicalPath()
    refpath = 'largest'
    examplepath = new File("${launchDir}/example_output").getCanonicalPath() // TODO: remove this when development is finished
    help = false
    version = false
    recombination = false
    curatedInput = true
    maxPartitionSize = 15000000  // same as ParSNP v1.5.6 default
    reinferTreeProg = 'fasttree'
    enable_conda_yml = false
}

timeline {
    enabled = true
    file    = "${params.outpath}/timeline.${trace_timestamp}.html"
}

report {
    enabled = true
    file    = "${params.outpath}/report.${trace_timestamp}.html"
}

trace {
    enabled = true
    fields  = 'task_id,name,status,exit,realtime,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar'
    file    = "${params.outpath}/trace.${trace_timestamp}.txt"
}

dag {
    enabled = true
    file    = "${params.outpath}/pipeline_dag.${trace_timestamp}.svg"
}

executor {
    queueSize = 2
}

// Increase conda env build duration
conda { createTimeout = "90 min" }

manifest {
    author = 'Christopher A. Gulvik'
    description = 'Identify SNPs from bacterial genome assemblies'
    mainScript = 'main.nf'
    nextflowVersion = '>=20.01.0'
    version = '1.0.0'
}
